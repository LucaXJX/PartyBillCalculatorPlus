"""
ç¬¬ä¸‰å±¤æ¨¡å‹è¨“ç·´ï¼šç´°ç²’åº¦é£Ÿç‰©åˆ†é¡ï¼ˆæŒ‰åœ‹å®¶/èœç³»ï¼‰
"""
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, callbacks
from tensorflow.keras.applications import MobileNetV2
from pathlib import Path
import json
import sys

# è¨­ç½® GPU å…§å­˜å¢é•·
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

def build_fine_grained_model(input_shape=(224, 224, 3), num_classes=10):
    """
    æ§‹å»ºç´°ç²’åº¦åˆ†é¡æ¨¡å‹
    
    Args:
        input_shape: è¼¸å…¥åœ–åƒå½¢ç‹€
        num_classes: åˆ†é¡æ•¸é‡
        
    Returns:
        ç·¨è­¯å¥½çš„æ¨¡å‹
    """
    base_model = MobileNetV2(
        input_shape=input_shape,
        include_top=False,
        weights='imagenet'
    )
    
    base_model.trainable = False
    
    model = keras.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(512, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy', 'top_k_categorical_accuracy']
    )
    
    return model

def load_data(data_dir):
    """åŠ è¼‰è¨“ç·´æ•¸æ“š"""
    train_datagen = keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        rotation_range=30,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True,
        zoom_range=0.2,
        fill_mode='nearest',
        validation_split=0.2
    )
    
    train_generator = train_datagen.flow_from_directory(
        data_dir,
        target_size=(224, 224),
        batch_size=32,
        class_mode='categorical',
        subset='training'
    )
    
    val_generator = train_datagen.flow_from_directory(
        data_dir,
        target_size=(224, 224),
        batch_size=32,
        class_mode='categorical',
        subset='validation'
    )
    
    return train_generator, val_generator

def train_level3(country='chinese'):
    """
    è¨“ç·´ç¬¬ä¸‰å±¤æ¨¡å‹ï¼ˆæŒ‰åœ‹å®¶/èœç³»ï¼‰
    
    Args:
        country: åœ‹å®¶/èœç³»åç¨±ï¼ˆå¦‚ 'chinese', 'japanese'ï¼‰
    """
    print(f"ğŸš€ é–‹å§‹è¨“ç·´ç¬¬ä¸‰å±¤æ¨¡å‹ï¼š{country} ç´°ç²’åº¦åˆ†é¡")
    
    # ç²å–é …ç›®æ ¹ç›®éŒ„
    script_dir = Path(__file__).resolve().parent  # train/
    service_dir = script_dir.parent  # food-recognition-service/
    project_root = service_dir.parent  # PartyBillCalculator/
    data_dir = project_root / 'data' / 'level3-fine-grained' / country
    model_dir = project_root / 'models' / 'level3' / country
    model_dir.mkdir(parents=True, exist_ok=True)
    
    if not data_dir.exists():
        print(f"âŒ æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {data_dir}")
        print(f"è«‹å…ˆæº–å‚™ {country} çš„è¨“ç·´æ•¸æ“š")
        return
    
    print("ğŸ“¦ åŠ è¼‰è¨“ç·´æ•¸æ“š...")
    train_gen, val_gen = load_data(str(data_dir))
    
    num_classes = len(train_gen.class_indices)
    print(f"åˆ†é¡æ•¸é‡: {num_classes}")
    print(f"é¡åˆ¥: {train_gen.class_indices}")
    print(f"è¨“ç·´æ¨£æœ¬æ•¸: {train_gen.samples}")
    print(f"é©—è­‰æ¨£æœ¬æ•¸: {val_gen.samples}")
    
    # ä¿å­˜é¡åˆ¥æ˜ å°„
    with open(model_dir / 'class_indices.json', 'w') as f:
        json.dump(train_gen.class_indices, f, indent=2)
    
    print("ğŸ—ï¸  æ§‹å»ºæ¨¡å‹...")
    model = build_fine_grained_model(num_classes=num_classes)
    model.summary()
    
    callbacks_list = [
        callbacks.ModelCheckpoint(
            str(model_dir / 'best_model.h5'),
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        ),
        callbacks.EarlyStopping(
            monitor='val_accuracy',
            patience=5,  # å¢åŠ  patience åˆ° 5ï¼Œçµ¦æ¨¡å‹æ›´å¤šæ™‚é–“æ”¶æ–‚
            restore_best_weights=True,
            verbose=1,
            min_delta=0.0005  # é™ä½æœ€å°æ”¹å–„é˜ˆå€¼ï¼Œå…è¨±æ›´å°çš„æ”¹å–„
        ),
        callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,  # å¢åŠ  patience åˆ° 3ï¼Œé¿å…éæ—©é™ä½å­¸ç¿’ç‡
            verbose=1,
            min_delta=0.0005  # é™ä½æœ€å°æ”¹å–„é˜ˆå€¼
        )
    ]
    
    print("[INFO] é–‹å§‹è¨“ç·´...")
    print("[INFO] ä½¿ç”¨æ—©åœæ©Ÿåˆ¶ï¼šå¦‚æœé©—è­‰æº–ç¢ºç‡5å€‹epochæ²’æœ‰æå‡ï¼Œå°‡è‡ªå‹•åœæ­¢è¨“ç·´")
    # CPU è¨“ç·´é…ç½®ï¼šä½¿ç”¨å¤šç·šç¨‹åŠ é€Ÿæ•¸æ“šåŠ è¼‰
    history = model.fit(
        train_gen,
        epochs=30,  # å¢åŠ åˆ°30å€‹epochï¼Œçµ¦æ¨¡å‹æ›´å¤šè¨“ç·´æ©Ÿæœƒ
        validation_data=val_gen,
        callbacks=callbacks_list,
        workers=4,  # CPU å¤šç·šç¨‹æ•¸æ“šåŠ è¼‰
        use_multiprocessing=False,  # Windows ä¸Šå»ºè­°è¨­ç‚º False
        verbose=1
    )
    
    print("[INFO] ä¿å­˜æ¨¡å‹...")
    model.save(str(model_dir / 'final_model'))
    
    # ä¿å­˜è¨“ç·´æ­·å²ï¼ˆè½‰æ› numpy é¡å‹ç‚º Python åŸç”Ÿé¡å‹ï¼‰
    import numpy as np
    def convert_to_serializable(obj):
        """å°‡ numpy é¡å‹è½‰æ›ç‚º Python åŸç”Ÿé¡å‹"""
        if isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: convert_to_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(item) for item in obj]
        return obj
    
    serializable_history = convert_to_serializable(history.history)
    with open(model_dir / 'training_history.json', 'w') as f:
        json.dump(serializable_history, f, indent=2)
    
    print("âœ… è¨“ç·´å®Œæˆï¼")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {model_dir / 'final_model'}")

if __name__ == '__main__':
    # å¯ä»¥é€šéå‘½ä»¤è¡Œåƒæ•¸æŒ‡å®šåœ‹å®¶
    country = sys.argv[1] if len(sys.argv) > 1 else 'chinese'
    train_level3(country)


