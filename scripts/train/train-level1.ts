/**
 * ç¬¬ä¸€å±¤æ¨¡å‹è¨“ç·´è…³æœ¬
 * è¨“ç·´é£Ÿç‰©æª¢æ¸¬æ¨¡å‹ï¼ˆFood / Non-Food äºŒåˆ†é¡ï¼‰
 */

import * as tf from "@tensorflow/tfjs-node-gpu"; // æˆ– @tensorflow/tfjs-node
import path from "path";
import { fileURLToPath } from "url";
import fs from "fs-extra";
import {
  DataLoader,
  buildFoodDetectionModel,
  compileModel,
  TensorAugmentation,
} from "../../server/food-recognition/training/index.js";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// è¨“ç·´é…ç½®
const config = {
  dataPath: path.join(__dirname, "../../data/level1-food-detection"),
  batchSize: 32,
  imageSize: [224, 224] as [number, number],
  epochs: 20,
  learningRate: 0.001,
  validationSplit: 0.15,
  testSplit: 0.15,
  modelSavePath: path.join(__dirname, "../../models/level1"),
  earlyStoppingPatience: 5,
};

async function trainLevel1() {
  console.log("ğŸš€ é–‹å§‹è¨“ç·´ç¬¬ä¸€å±¤æ¨¡å‹ï¼ˆé£Ÿç‰©æª¢æ¸¬ï¼‰...");
  console.log("é…ç½®:", config);

  try {
    // 1. æª¢æŸ¥æ•¸æ“šç›®éŒ„
    if (!(await fs.pathExists(config.dataPath))) {
      throw new Error(`æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: ${config.dataPath}`);
    }

    // 2. åŠ è¼‰æ•¸æ“š
    console.log("\nğŸ“‚ åŠ è¼‰æ•¸æ“šé›†...");
    const dataLoader = new DataLoader();
    const { trainDataset, valDataset, testDataset, classNames, numClasses } =
      await dataLoader.loadBinaryDataset({
        dataPath: config.dataPath,
        batchSize: config.batchSize,
        imageSize: config.imageSize,
        validationSplit: config.validationSplit,
        testSplit: config.testSplit,
        shuffle: true,
      });

    console.log(`âœ… æ•¸æ“šåŠ è¼‰å®Œæˆ`);
    console.log(`   é¡åˆ¥: ${classNames.join(", ")}`);
    console.log(`   é¡åˆ¥æ•¸: ${numClasses}`);

    // 3. æ‡‰ç”¨æ•¸æ“šå¢å¼·
    console.log("\nğŸ”„ æ‡‰ç”¨æ•¸æ“šå¢å¼·...");
    const augmentation = new TensorAugmentation();
    const augmentedTrainDataset = augmentation.augmentDataset(trainDataset, {
      flipHorizontal: true,
      rotate: true,
      brightness: true,
      contrast: true,
    });
    console.log("âœ… æ•¸æ“šå¢å¼·å®Œæˆ");

    // 4. æ§‹å»ºæ¨¡å‹
    console.log("\nğŸ—ï¸  æ§‹å»ºæ¨¡å‹...");
    const model = buildFoodDetectionModel(config.imageSize);
    compileModel(model, "binary", config.learningRate);
    console.log("âœ… æ¨¡å‹æ§‹å»ºå®Œæˆ");
    model.summary();

    // 5. è¨“ç·´é…ç½®
    let bestValLoss = Infinity;
    let patienceCounter = 0;
    let bestWeights: tf.WeightsManifestEntry[] | null = null;

    // 6. è¨“ç·´æ¨¡å‹
    console.log("\nğŸ¯ é–‹å§‹è¨“ç·´...");
    const history: {
      epoch: number;
      loss: number;
      acc: number;
      valLoss: number;
      valAcc: number;
    }[] = [];

    for (let epoch = 0; epoch < config.epochs; epoch++) {
      console.log(`\nEpoch ${epoch + 1}/${config.epochs}`);

      // è¨“ç·´ä¸€å€‹ epoch
      const epochHistory = await model.fitDataset(augmentedTrainDataset, {
        epochs: 1,
        validationData: valDataset,
        callbacks: {
          onEpochEnd: async (epochNum, logs) => {
            const loss = logs?.loss as number;
            const acc = logs?.acc as number;
            const valLoss = logs?.val_loss as number;
            const valAcc = logs?.val_acc as number;

            console.log(
              `  Loss: ${loss?.toFixed(4)}, Acc: ${acc?.toFixed(4)}, ` +
                `Val Loss: ${valLoss?.toFixed(4)}, Val Acc: ${valAcc?.toFixed(4)}`
            );

            history.push({
              epoch: epochNum + 1,
              loss: loss || 0,
              acc: acc || 0,
              valLoss: valLoss || 0,
              valAcc: valAcc || 0,
            });

            // Early stopping
            if (valLoss < bestValLoss) {
              bestValLoss = valLoss;
              patienceCounter = 0;
              // ä¿å­˜æœ€ä½³æ¬Šé‡
              bestWeights = await model.getWeights();
              console.log("  âœ… æ‰¾åˆ°æ›´å¥½çš„æ¨¡å‹ï¼Œä¿å­˜æ¬Šé‡");
            } else {
              patienceCounter++;
              if (patienceCounter >= config.earlyStoppingPatience) {
                console.log(
                  `  â¹ï¸  æ—©åœè§¸ç™¼ï¼ˆ${patienceCounter} å€‹ epoch ç„¡æ”¹å–„ï¼‰`
                );
                return;
              }
            }
          },
        },
      });

      // æª¢æŸ¥æ˜¯å¦æ‡‰è©²æ—©åœ
      if (patienceCounter >= config.earlyStoppingPatience) {
        break;
      }
    }

    // 7. æ¢å¾©æœ€ä½³æ¬Šé‡
    if (bestWeights) {
      console.log("\nğŸ“¥ æ¢å¾©æœ€ä½³æ¬Šé‡...");
      model.setWeights(bestWeights);
    }

    // 8. è©•ä¼°æ¨¡å‹
    console.log("\nğŸ“Š è©•ä¼°æ¨¡å‹...");
    const testResults = await model.evaluateDataset(testDataset);
    const testLoss = (await (testResults[0] as tf.Scalar).data())[0];
    const testAcc = (await (testResults[1] as tf.Scalar).data())[0];
    console.log(`æ¸¬è©¦é›† Loss: ${testLoss.toFixed(4)}`);
    console.log(`æ¸¬è©¦é›† Acc: ${testAcc.toFixed(4)}`);

    // æ¸…ç†è©•ä¼°å¼µé‡
    testResults.forEach((tensor) => tensor.dispose());

    // 9. ä¿å­˜æ¨¡å‹
    console.log("\nğŸ’¾ ä¿å­˜æ¨¡å‹...");
    await fs.ensureDir(config.modelSavePath);
    await model.save(`file://${config.modelSavePath}`);
    console.log(`âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: ${config.modelSavePath}`);

    // 10. ä¿å­˜è¨“ç·´æ­·å²
    const historyPath = path.join(config.modelSavePath, "training-history.json");
    await fs.writeJSON(historyPath, history, { spaces: 2 });
    console.log(`âœ… è¨“ç·´æ­·å²å·²ä¿å­˜åˆ°: ${historyPath}`);

    console.log("\nğŸ‰ è¨“ç·´å®Œæˆï¼");
  } catch (error) {
    console.error("\nâŒ è¨“ç·´å¤±æ•—:", error);
    process.exit(1);
  }
}

// åŸ·è¡Œè¨“ç·´
trainLevel1();




