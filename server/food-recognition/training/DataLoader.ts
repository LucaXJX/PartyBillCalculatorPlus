import * as tf from "@tensorflow/tfjs-node";
import fs from "fs-extra";
import path from "path";
import sharp from "sharp";

/**
 * æ•¸æ“šé›†é…ç½®
 */
export interface DatasetConfig {
  dataPath: string;
  batchSize: number;
  imageSize: [number, number];
  validationSplit?: number; // é©—è­‰é›†æ¯”ä¾‹ï¼Œé»˜èª 0.15
  testSplit?: number; // æ¸¬è©¦é›†æ¯”ä¾‹ï¼Œé»˜èª 0.15
  shuffle?: boolean; // æ˜¯å¦æ‰“äº‚æ•¸æ“š
}

/**
 * åœ–åƒæ¨™ç±¤å°
 */
export interface ImageLabelPair {
  image: tf.Tensor4D;
  label: tf.Tensor;
  filePath: string;
}

/**
 * æ•¸æ“šåŠ è¼‰å™¨
 * è² è²¬å¾æ–‡ä»¶ç³»çµ±åŠ è¼‰åœ–åƒæ•¸æ“šé›†ä¸¦è½‰æ›ç‚º TensorFlow.js æ ¼å¼
 */
export class DataLoader {
  /**
   * å¾ç›®éŒ„çµæ§‹åŠ è¼‰åˆ†é¡æ•¸æ“šé›†
   * ç›®éŒ„çµæ§‹ï¼šdataPath/class1/, dataPath/class2/, ...
   * @param config æ•¸æ“šé›†é…ç½®
   * @returns è¨“ç·´é›†ã€é©—è­‰é›†ã€æ¸¬è©¦é›†å’Œé¡åˆ¥æ˜ å°„
   */
  async loadClassificationDataset(config: DatasetConfig): Promise<{
    trainDataset: tf.data.Dataset<{ xs: tf.Tensor4D; ys: tf.Tensor }>;
    valDataset: tf.data.Dataset<{ xs: tf.Tensor4D; ys: tf.Tensor }>;
    testDataset: tf.data.Dataset<{ xs: tf.Tensor4D; ys: tf.Tensor }>;
    classNames: string[];
    numClasses: number;
  }> {
    const {
      dataPath,
      batchSize,
      imageSize,
      validationSplit = 0.15,
      testSplit = 0.15,
      shuffle = true,
    } = config;

    // è®€å–æ‰€æœ‰é¡åˆ¥ç›®éŒ„
    const entries = await fs.readdir(dataPath, { withFileTypes: true });
    const classDirs = entries
      .filter((entry: fs.Dirent) => entry.isDirectory())
      .map((entry: fs.Dirent) => entry.name)
      .sort();

    if (classDirs.length === 0) {
      throw new Error(`æ•¸æ“šç›®éŒ„ä¸­æ²’æœ‰æ‰¾åˆ°é¡åˆ¥æ–‡ä»¶å¤¾: ${dataPath}`);
    }

    console.log(`ğŸ“ æ‰¾åˆ° ${classDirs.length} å€‹é¡åˆ¥: ${classDirs.join(", ")}`);

    // æ”¶é›†æ‰€æœ‰åœ–åƒæ–‡ä»¶
    const allImages: Array<{ path: string; classIndex: number }> = [];

    for (let i = 0; i < classDirs.length; i++) {
      const classDir = path.join(dataPath, classDirs[i]);
      const files = await fs.readdir(classDir);
      const imageFiles = files.filter((file: string) =>
        /\.(jpg|jpeg|png|bmp|webp)$/i.test(file)
      );

      for (const file of imageFiles) {
        allImages.push({
          path: path.join(classDir, file),
          classIndex: i,
        });
      }

      console.log(`  ${classDirs[i]}: ${imageFiles.length} å¼µåœ–ç‰‡`);
    }

    if (allImages.length === 0) {
      throw new Error("æ²’æœ‰æ‰¾åˆ°ä»»ä½•åœ–åƒæ–‡ä»¶");
    }

    // æ‰“äº‚æ•¸æ“š
    if (shuffle) {
      this.shuffleArray(allImages);
    }

    // åŠƒåˆ†æ•¸æ“šé›†
    const total = allImages.length;
    const testCount = Math.floor(total * testSplit);
    const valCount = Math.floor(total * validationSplit);
    const trainCount = total - testCount - valCount;

    const testImages = allImages.slice(0, testCount);
    const valImages = allImages.slice(testCount, testCount + valCount);
    const trainImages = allImages.slice(testCount + valCount);

    console.log(
      `ğŸ“Š æ•¸æ“šåŠƒåˆ†: è¨“ç·´é›† ${trainImages.length}, é©—è­‰é›† ${valImages.length}, æ¸¬è©¦é›† ${testImages.length}`
    );

    // å‰µå»ºæ•¸æ“šé›†
    const trainDataset = this.createDataset(trainImages, imageSize, classDirs.length).batch(batchSize) as tf.data.Dataset<{ xs: tf.Tensor4D; ys: tf.Tensor }>;
    const valDataset = this.createDataset(valImages, imageSize, classDirs.length).batch(batchSize) as tf.data.Dataset<{ xs: tf.Tensor4D; ys: tf.Tensor }>;
    const testDataset = this.createDataset(testImages, imageSize, classDirs.length).batch(batchSize) as tf.data.Dataset<{ xs: tf.Tensor4D; ys: tf.Tensor }>;

    return {
      trainDataset,
      valDataset,
      testDataset,
      classNames: classDirs,
      numClasses: classDirs.length,
    };
  }

  /**
   * å¾ç›®éŒ„çµæ§‹åŠ è¼‰äºŒåˆ†é¡æ•¸æ“šé›†ï¼ˆç”¨æ–¼ç¬¬ä¸€å±¤ï¼šé£Ÿç‰©/éé£Ÿç‰©ï¼‰
   * ç›®éŒ„çµæ§‹ï¼šdataPath/food/, dataPath/non-food/
   */
  async loadBinaryDataset(config: DatasetConfig): Promise<{
    trainDataset: tf.data.Dataset<{ xs: tf.Tensor4D; ys: tf.Tensor }>;
    valDataset: tf.data.Dataset<{ xs: tf.Tensor4D; ys: tf.Tensor }>;
    testDataset: tf.data.Dataset<{ xs: tf.Tensor4D; ys: tf.Tensor }>;
  }> {
    const result = await this.loadClassificationDataset(config);
    return {
      trainDataset: result.trainDataset,
      valDataset: result.valDataset,
      testDataset: result.testDataset,
    };
  }

  /**
   * å‰µå»º TensorFlow.js æ•¸æ“šé›†
   */
  private createDataset(
    images: Array<{ path: string; classIndex: number }>,
    imageSize: [number, number],
    numClasses: number
  ): tf.data.Dataset<{ xs: tf.Tensor4D; ys: tf.Tensor }> {
    // ä½¿ç”¨ generator ä¾†æ”¯æŒç•°æ­¥åœ–åƒåŠ è¼‰
    const generator = async function* () {
      for (const item of images) {
        try {
          // è®€å–åœ–åƒæ–‡ä»¶
          const buffer = await fs.readFile(item.path);

          // è§£ç¢¼åœ–åƒ
          const imageTensor = tf.node.decodeImage(buffer, 3);

          // èª¿æ•´å¤§å°
          const resized = tf.image.resizeBilinear(imageTensor, imageSize);

          // æ­¸ä¸€åŒ–åˆ° [0, 1]
          const normalized = resized.div(255.0);

          // æ·»åŠ æ‰¹æ¬¡ç¶­åº¦
          const batched = normalized.expandDims(0) as tf.Tensor4D;

          // å‰µå»ºæ¨™ç±¤ï¼ˆone-hot ç·¨ç¢¼ï¼‰
          const label = tf.oneHot(tf.scalar(item.classIndex, "int32"), numClasses);

          // æ¸…ç†ä¸­é–“å¼µé‡
          imageTensor.dispose();
          resized.dispose();
          normalized.dispose();

          yield {
            xs: batched,
            ys: label,
          };
        } catch (error) {
          console.error(`åŠ è¼‰åœ–åƒå¤±æ•— ${item.path}:`, error);
          // è·³ééŒ¯èª¤çš„åœ–åƒ
          continue;
        }
      }
    };

    return tf.data.generator(generator) as tf.data.Dataset<{ xs: tf.Tensor4D; ys: tf.Tensor }>;
  }

  /**
   * ç²å–é¡åˆ¥æ•¸é‡
   */
  async getNumClasses(dataPath: string): Promise<number> {
    const entries = await fs.readdir(dataPath, { withFileTypes: true });
    const classDirs = entries.filter((entry: fs.Dirent) => entry.isDirectory());
    return classDirs.length;
  }

  /**
   * ç²å–é¡åˆ¥åç¨±åˆ—è¡¨
   */
  async getClassNames(dataPath: string): Promise<string[]> {
    const entries = await fs.readdir(dataPath, { withFileTypes: true });
    return entries
      .filter((entry: fs.Dirent) => entry.isDirectory())
      .map((entry: fs.Dirent) => entry.name)
      .sort();
  }

  /**
   * ç²å–æ•¸æ“šé›†çµ±è¨ˆä¿¡æ¯
   */
  async getDatasetStats(dataPath: string): Promise<{
    totalClasses: number;
    totalImages: number;
    classStats: Array<{ className: string; count: number }>;
  }> {
    const entries = await fs.readdir(dataPath, { withFileTypes: true });
    const classDirs = entries
      .filter((entry: fs.Dirent) => entry.isDirectory())
      .map((entry: fs.Dirent) => entry.name)
      .sort();

    const classStats: Array<{ className: string; count: number }> = [];
    let totalImages = 0;

    for (const className of classDirs) {
      const classPath = path.join(dataPath, className);
      const files = await fs.readdir(classPath);
      const imageFiles = files.filter((file: string) =>
        /\.(jpg|jpeg|png|bmp|webp)$/i.test(file)
      );
      const count = imageFiles.length;
      classStats.push({ className, count });
      totalImages += count;
    }

    return {
      totalClasses: classDirs.length,
      totalImages,
      classStats,
    };
  }

  /**
   * æ‰“äº‚æ•¸çµ„ï¼ˆFisher-Yates æ´—ç‰Œç®—æ³•ï¼‰
   */
  private shuffleArray<T>(array: T[]): void {
    for (let i = array.length - 1; i > 0; i--) {
      const j = Math.floor(Math.random() * (i + 1));
      [array[i], array[j]] = [array[j], array[i]];
    }
  }

  /**
   * é©—è­‰åœ–åƒæ–‡ä»¶
   */
  async validateImageFile(filePath: string): Promise<boolean> {
    try {
      const metadata = await sharp(filePath).metadata();
      return (
        metadata.width !== undefined &&
        metadata.height !== undefined &&
        metadata.width > 0 &&
        metadata.height > 0
      );
    } catch {
      return false;
    }
  }
}

